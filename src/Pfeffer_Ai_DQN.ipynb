{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "\n",
    "from src.Game import Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(pd.__version__)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93360d80e4730fd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dc1af804c621f4d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bid_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(24 + 1 + 2 + 15,), name=\"input\"),\n",
    "    tf.keras.layers.Dense(64, activation='relu', name=\"hidden1\"),\n",
    "    tf.keras.layers.Dense(64, activation='relu', name=\"hidden2\"),\n",
    "    tf.keras.layers.Dense(10, name=\"output\"),\n",
    "], name=\"bid_model\")\n",
    "bid_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "bid_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5c15183fcd0a164",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "play_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4 + 24 + (6 * 4 * 24) + 16 + 20 + 14 + 30 + 30 + 6 + 2,), name=\"input\"),  # 722\n",
    "    tf.keras.layers.Dense(64, activation='relu', name=\"hidden1\"),\n",
    "    tf.keras.layers.Dense(64, activation='relu', name=\"hidden2\"),\n",
    "    tf.keras.layers.Dense(24, name=\"output\"),\n",
    "], name=\"play_model\")\n",
    "play_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "play_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b60721dce42c4b27",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BatchLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.batch_logs = []\n",
    "        self.total_batches_seen = 0\n",
    "        self.log_df = None\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.total_batches_seen += 1\n",
    "        self.batch_logs.append(\n",
    "            {'Iteration': self.total_batches_seen, 'Loss': logs['loss'], 'Accuracy': logs['accuracy']})\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Create the log DataFrame at the end of each epoch\n",
    "        self.log_df = pd.DataFrame(self.batch_logs)\n",
    "\n",
    "\n",
    "logger = BatchLogger()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de2369cb47d13c07",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "epochs = 1\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.1\n",
    "episodes = 1  # 2_000\n",
    "batch_size = 2 ** 4  #  8192  # 32\n",
    "gamma = 1  # 0.95"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d8c1e08170f97cb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "game = Game(bid_model, play_model)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()  # Get the current time\n",
    "for e in range(episodes):\n",
    "    if e % 100 == 0:\n",
    "        print(f\"Episode {e} | epsilon {epsilon}\")\n",
    "\n",
    "    game.reset()\n",
    "    game.play_game(max_rounds=4)\n",
    "\n",
    "    # Prepare dataset using replay buffer\n",
    "    # dataset = game.players[0].bid_replay_buffer.plot_as_dataset(batch_size)\n",
    "    dataset = game.players[0].bid_replay_buffer.as_dataset(\n",
    "        num_parallel_calls=3,\n",
    "        sample_batch_size=batch_size,\n",
    "        num_steps=2\n",
    "    ).prefetch(3)\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for trajectories in iterator:\n",
    "        # encoded_state_dict = trajectories[0].observation\n",
    "        # flattened_encoded_state = tf.concat([tf.reshape(tensor, [-1]) for tensor in encoded_state_dict.values()], 0)\n",
    "        # flattened_encoded_state = tf.reshape(flattened_encoded_state, (-1, 42))\n",
    "        \n",
    "        encoded_states_dict = trajectories[0].observation\n",
    "        current_state_dict = {k: v[:, 0, :] for k, v in encoded_states_dict.items()}\n",
    "        next_state_dict = {k: v[:, 1, :] for k, v in encoded_states_dict.items()}\n",
    "        \n",
    "        flattened_current_state = tf.concat([tf.reshape(tensor, [-1]) for tensor in current_state_dict.values()], 0)\n",
    "        flattened_next_state = tf.concat([tf.reshape(tensor, [-1]) for tensor in next_state_dict.values()], 0)\n",
    "        \n",
    "        flattened_current_state = tf.reshape(flattened_current_state, (-1, 42)).numpy()\n",
    "        flattened_next_state = tf.reshape(flattened_next_state, (-1, 42)).numpy()\n",
    "        \n",
    "        actions, rewards = trajectories[0].action[:, 0], trajectories[0].reward[:, 0]\n",
    "        dones = trajectories[0].step_type[:, 1]  #LAST = end of episode\n",
    "\n",
    "        bid_targets = game.players[0].bid_model.predict(flattened_current_state)\n",
    "        next_q_values = game.players[0].bid_model.predict(flattened_next_state)\n",
    "        \n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                bid_targets[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                bid_targets[i][actions[i]] = rewards[i] + gamma * np.amax(next_q_values[i])\n",
    "\n",
    "        game.players[0].bid_model.fit(flattened_current_state, bid_targets, epochs=epochs, verbose=0, callbacks=[logger])\n",
    "\n",
    "    # Replay Bids\n",
    "    # trajectories = game.players[0].bid_replay_buffer.gather_all()\n",
    "    # states, actions, rewards, next_states, dones = (trajectories.observation[:, 0],\n",
    "    #                                                 trajectories.action,\n",
    "    #                                                 trajectories.reward,\n",
    "    #                                                 trajectories.observation[:, 1],\n",
    "    #                                                 trajectories.step_type[:, 1])\n",
    "    # bid_targets = game.players[0].bid_model.predict(states)\n",
    "    # next_q_values = game.players[0].bid_model.predict(next_states)\n",
    "    # \n",
    "    # for i, done in enumerate(dones):\n",
    "    #     if done:\n",
    "    #         bid_targets[i][actions[i]] = rewards[i]\n",
    "    #     else:\n",
    "    #         bid_targets[i][actions[i]] = rewards[i] + gamma * np.amax(next_q_values[i])\n",
    "    # \n",
    "    # game.players[0].bid_model.fit(states, bid_targets, epochs=epochs, verbose=0, callbacks=[logger])\n",
    "\n",
    "end_time = time.time()  # Get the current time again after your code has run\n",
    "execution_time = end_time - start_time  # Calculate the difference\n",
    "print(f\"The execution time was: {execution_time} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d084b341aa9d0b0",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
